{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Notebook plotting ---\n",
    "%matplotlib inline\n",
    "\n",
    "# --- Standard library ---\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Third-party libraries ---\n",
    "import dagshub\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import io\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.plots import plot_objective, plot_histogram\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, make_scorer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.stats import randint\n",
    "\n",
    "# --- Project-specific imports ---\n",
    "from src import dataset, preprocess\n",
    "from src.config import RAW_DATA_DIR, INTERIM_DATA_DIR\n",
    "from src.modeling import train, predict\n",
    "\n",
    "# --- MLflow / DagsHub init ---\n",
    "dagshub.init(repo_owner=\"joscha0610\", repo_name=\"earthquake-damage-ml\", mlflow=True)\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# --- Figure output directory ---\n",
    "FIG_DIR = Path(\"../reports/figures\")\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competition (train/test/labels)\n",
    "X, y, test = dataset.load_competition_raw(RAW_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 One-Hot-Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess.one_hot_encode(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train Test Split\n",
    "Split data in 80% trainings data and 20% validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = preprocess.split_train_val(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = train.build_rf_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hyperparameter tuning with Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Define parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {'randomforestclassifier__n_estimators': [100, 250, 500, 1000],\n",
    "#             'randomforestclassifier__min_samples_leaf': [1, 2, 4],\n",
    "#             'randomforestclassifier__max_features': [20, 25, 30, 35, 40],\n",
    "#             'randomforestclassifier__max_depth': [10, 25, 50],\n",
    "#             'randomforestclassifier__min_samples_split': [20, 25, 30, 35],\n",
    "#             'randomforestclassifier__criterion': ['gini', 'entropy']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Tune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Set up Grid Search\n",
    "# grid = GridSearchCV(\n",
    "#     estimator=pipe,\n",
    "#     param_grid=param_grid,\n",
    "#     scoring=make_scorer(f1_score, average=\"micro\"),\n",
    "#     cv=3\n",
    "# )\n",
    "\n",
    "# # 2. Create MLflow experiment\n",
    "# experiment_name = \"rf_grid_search\"\n",
    "# mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# # 3. Train model with Grid Search\n",
    "# mlflow.autolog(log_models=False)  # we'll log the best model manually\n",
    "\n",
    "# with mlflow.start_run(run_name=\"grid_search_rf\") as run:\n",
    "#     # Fit search\n",
    "#     grid.fit(X_train, y_train[\"damage_grade\"].values.ravel())\n",
    "\n",
    "#     # Extract best results\n",
    "#     best_model = grid.best_estimator_\n",
    "#     best_score = grid.best_score_\n",
    "#     best_params = grid.best_params_\n",
    "\n",
    "#     # Log metrics and parameters\n",
    "#     mlflow.log_metric(\"best_cv_f1_micro\", best_score)\n",
    "#     mlflow.log_params(best_params)\n",
    "\n",
    "#     # Log best model explicitly\n",
    "#     mlflow.sklearn.log_model(\n",
    "#         sk_model=best_model,\n",
    "#         artifact_path=\"sklearn-model\",\n",
    "#         input_example=X_train.head(5),\n",
    "#         registered_model_name=\"rf_grid_search_best\"\n",
    "#     )\n",
    "\n",
    "# # 4. Output results\n",
    "# print(f\"Best parameters: {best_params}\")\n",
    "# print(f\"Best CV score: {best_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Hyperparameter tuning with Randomized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Define parameter distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_dist = {'randomforestclassifier__n_estimators': randint(50,500),\n",
    "#               'randomforestclassifier__min_samples_leaf': randint(1,10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1) Set up RandomizedSearchCV\n",
    "# rs = RandomizedSearchCV(\n",
    "#     estimator=pipe,\n",
    "#     param_distributions=param_dist,\n",
    "#     n_iter=30,\n",
    "#     cv=3,\n",
    "#     scoring=make_scorer(f1_score, average=\"micro\"),\n",
    "#     random_state=123,\n",
    "#     n_jobs=-1,\n",
    "#     refit=True,\n",
    "#     return_train_score=False,\n",
    "# )\n",
    "\n",
    "# # 3) MLflow experiment + run\n",
    "# mlflow.set_experiment(\"rf-randomsearch\")\n",
    "# mlflow.sklearn.autolog(\n",
    "#     log_models=False,            # avoid duplicate model; we log manually below\n",
    "#     log_model_signatures=True,\n",
    "#     log_input_examples=True\n",
    "# )\n",
    "\n",
    "# with mlflow.start_run(run_name=\"random_search_rf\"):\n",
    "#     rs.fit(X_train, y_train[\"damage_grade\"].values.ravel())\n",
    "\n",
    "#     best_model  = rs.best_estimator_\n",
    "#     best_score  = float(rs.best_score_)\n",
    "#     best_params = rs.best_params_\n",
    "\n",
    "#     # Metrics\n",
    "#     mlflow.log_metric(\"best_cv_f1_micro\", best_score)\n",
    "#     val_f1 = float(rs.score(X_val, y_val[\"damage_grade\"].values.ravel()))\n",
    "#     mlflow.log_metric(\"val_f1_micro\", val_f1)\n",
    "\n",
    "#     # Params â€” use a separate namespace to avoid collisions with autolog\n",
    "#     mlflow.log_params({f\"best__{k}\": str(v) for k, v in best_params.items()})\n",
    "#     mlflow.log_dict({k: str(v) for k, v in param_dist.items()}, \"random_search_space.json\")\n",
    "\n",
    "#     # Full CV table as artifact\n",
    "#     if hasattr(rs, \"cv_results_\"):\n",
    "#         pd.DataFrame(rs.cv_results_).to_csv(\"cv_results_random_search.csv\", index=False)\n",
    "#         mlflow.log_artifact(\"cv_results_random_search.csv\")\n",
    "\n",
    "#     # Log the best model explicitly\n",
    "#     mlflow.sklearn.log_model(\n",
    "#         sk_model=best_model,\n",
    "#         artifact_path=\"sklearn-model-best\",\n",
    "#         input_example=X_train.head(5)\n",
    "#     )\n",
    "\n",
    "# print(f\"Best parameters: {best_params}\")\n",
    "# print(f\"Best CV score: {best_score:.4f}\")\n",
    "# print(f\"Validation f1_micro: {val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Sequential Model-Based Optimization (SMBO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Define search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_space = {\n",
    "#     'randomforestclassifier__n_estimators': (100, 1000),\n",
    "#     'randomforestclassifier__max_depth': (1, 50),\n",
    "#     'randomforestclassifier__min_samples_split': (2, 100),\n",
    "#     'randomforestclassifier__min_samples_leaf': (1, 50),\n",
    "#     'randomforestclassifier__criterion': ['gini', 'entropy'],\n",
    "#     'randomforestclassifier__max_features': (20, 39)\n",
    "#     }\n",
    "\n",
    "search_space = {\n",
    "    'randomforestclassifier__n_estimators': (10, 20),\n",
    "    'randomforestclassifier__max_depth': (1, 10),\n",
    "    'randomforestclassifier__min_samples_split': (2, 20),\n",
    "    'randomforestclassifier__min_samples_leaf': (1, 10),\n",
    "    'randomforestclassifier__criterion': ['gini', 'entropy'],\n",
    "    'randomforestclassifier__max_features': (20, 39)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"rf-bayessearch\")\n",
    "\n",
    "# ---- your search -----------------------------------------------------------\n",
    "opt = BayesSearchCV(\n",
    "    pipe,\n",
    "    search_space,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    scoring=make_scorer(f1_score, average=\"micro\"),\n",
    "    random_state=123,\n",
    ")\n",
    "\n",
    "with mlflow.start_run(run_name=\"bayes_rf\"):\n",
    "    # Fit\n",
    "    np.int = int\n",
    "    opt.fit(X_train, y_train[\"damage_grade\"].values.ravel())\n",
    "\n",
    "    # Metrics\n",
    "    best_cv = float(opt.best_score_)\n",
    "    val_f1 = float(opt.score(X_val, y_val[\"damage_grade\"].values.ravel()))\n",
    "    mlflow.log_metric(\"best_cv_f1_micro\", best_cv)\n",
    "    mlflow.log_metric(\"val_f1_micro\", val_f1)\n",
    "\n",
    "    # Log search space for reproducibility\n",
    "    mlflow.log_dict(search_space, \"search_space.json\")\n",
    "\n",
    "    # Params\n",
    "    mlflow.log_params(opt.best_params_)\n",
    "\n",
    "    # ---- Full CV results as artifact ----\n",
    "    if hasattr(opt, \"cv_results_\"):\n",
    "        df = pd.DataFrame(opt.cv_results_)\n",
    "        buffer = io.StringIO()\n",
    "        df.to_csv(buffer, index=False)\n",
    "        mlflow.log_text(buffer.getvalue(), artifact_file=\"cv_results_bayes.csv\")\n",
    "\n",
    "    # Log the best estimator as a clear, separate artifact\n",
    "    best_est = opt.best_estimator_\n",
    "\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=best_est,\n",
    "        artifact_path=\"sklearn-model-best\",\n",
    "        input_example=(X_train.iloc[:5] if hasattr(X_train, \"iloc\") else X_train[:5]),\n",
    "        registered_model_name=\"rf_bayessearch\"\n",
    "    )\n",
    "\n",
    "# ---- Print summary ----\n",
    "print(f\"Best CV score: {best_cv:.4f}\")\n",
    "print(f\"Validation score: {val_f1:.4f}\")\n",
    "print(\"Best parameters:\", opt.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Plot the optimizer process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_objective(opt.optimizer_results_[0],\n",
    "                   dimensions=[\"n_estimators\", \"max_depth\", \"min_samples_split\", \"min_samples_leaf\", \"max_features\", \"criterion\"],)\n",
    "\n",
    "\n",
    "# Save figure\n",
    "fig_path = FIG_DIR / \"rf_bayesian_optimization.png\"\n",
    "plt.savefig(fig_path, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = opt.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importances = opt.best_estimator_.named_steps['randomforestclassifier'].feature_importances_\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "features_df = features_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Create horizontal bar plot\n",
    "plt.figure(figsize=(15, 20))\n",
    "plt.barh(features_df['Feature'], features_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances')\n",
    "plt.gca().invert_yaxis()  # show most important features at the top\n",
    "\n",
    "# Save figure\n",
    "fig_path = FIG_DIR / \"rf_feature_importance.png\"\n",
    "plt.savefig(fig_path, dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val[\"damage_grade\"].values.ravel(), val_preds)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[1, 2, 3], yticklabels=[1, 2, 3])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion matrix Random Forest')\n",
    "\n",
    "# Save figure\n",
    "fig_path = FIG_DIR / \"rf_confusion_matrix.png\"\n",
    "plt.savefig(fig_path, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = \"models:/rf_bayessearch/2\"\n",
    "model = mlflow.sklearn.load_model(model_uri)\n",
    "test_values_path = \"../data/raw/competition/test_values.csv\"\n",
    "output_path = \"../models/submission_rf_test.csv\"\n",
    "\n",
    "predict.create_submission(model=model, test_values_path=test_values_path, output_path=output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthquake-damage-ml-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
